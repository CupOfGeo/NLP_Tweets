{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>insult</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10360</td>\n",
       "      <td>10358</td>\n",
       "      <td>10360</td>\n",
       "      <td>10360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1573</td>\n",
       "      <td>866</td>\n",
       "      <td>6729</td>\n",
       "      <td>5673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2020-10-12</td>\n",
       "      <td>the-media</td>\n",
       "      <td>Fake News</td>\n",
       "      <td>The Fake News Networks, those that knowingly h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>45</td>\n",
       "      <td>1287</td>\n",
       "      <td>431</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date     target     insult  \\\n",
       "count        10360      10358      10360   \n",
       "unique        1573        866       6729   \n",
       "top     2020-10-12  the-media  Fake News   \n",
       "freq            45       1287        431   \n",
       "\n",
       "                                                    tweet  \n",
       "count                                               10360  \n",
       "unique                                               5673  \n",
       "top     The Fake News Networks, those that knowingly h...  \n",
       "freq                                                   16  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('trump_insult_tweets_2014_to_2021.csv',index_col='Unnamed: 0')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweets'] = df['tweet'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = np.array(df['clean_tweets'][df['clean_tweets'] != ''].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5254"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([len(tweet) for tweet in clean_tweets]).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = [x.replace('\"','') for x in clean_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10384\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "corpus = clean_tweets\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111, 45, 243, 27, 826, 1530, 4303, 5739, 6, 4304, 36, 706, 380, 19, 5740, 56, 16, 444, 73, 11, 17, 66, 7, 1384, 5741, 1217]\n",
      "['Can you believe this fool, Dr. Thomas Frieden of CDC, just stated, anyone with fever should be asked if they have been in West Africa DOPE']\n"
     ]
    }
   ],
   "source": [
    "token_list = tokenizer.texts_to_sequences([corpus[0]])[0]\n",
    "print(token_list)\n",
    "print([corpus[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_len = 5\n",
    "X_data = []\n",
    "y_data = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "#     for i in range(1, len(token_list)):\n",
    "#         n_gram_sequence = token_list[:i+1]\n",
    "#         input_sequences.append(n_gram_sequence)\n",
    "        \n",
    "    for j in range(len(token_list) - seq_len):\n",
    "        \n",
    "        in_seq = token_list[j : j + seq_len]\n",
    "        out_seq = token_list[j + seq_len]\n",
    "        X_data.append(in_seq)        \n",
    "        y_data.append(out_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     3, ..., 10381, 10383, 10384])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_patterns = len(X_data) #157566 seq_length = 5\n",
    "pd.DataFrame(y_data).iloc[:,0].sort_values().unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X_data, (n_patterns, seq_len, 1))\n",
    "X = X/float(len(tokenizer.word_index.items()))\n",
    "y_temp = pd.get_dummies(y_data)\n",
    "\n",
    "def f(x):\n",
    "    y_temp[x] = 0\n",
    "    \n",
    "[f(x) for x in list(range(1,total_words)) if (x not in y_temp.columns)]\n",
    "\n",
    "y = np.asarray(y_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((157566, 5, 1), (157566, 10384), 10384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, len(tokenizer.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y[0][1529])\n",
    "len(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(n_patterns+1, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(total_words, activation='softmax')) #outputs a on hot encoded row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model_weights_saved.hdf5\"\n",
    "#model.load_weights(filename)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1576/1576 [==============================] - 293s 182ms/step - loss: 7.1838 - accuracy: 0.0463\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.98056, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/10\n",
      "1576/1576 [==============================] - 283s 179ms/step - loss: 6.7853 - accuracy: 0.0469\n",
      "\n",
      "Epoch 00002: loss improved from 6.98056 to 6.79638, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/10\n",
      "1576/1576 [==============================] - 315s 200ms/step - loss: 6.7633 - accuracy: 0.0486\n",
      "\n",
      "Epoch 00003: loss improved from 6.79638 to 6.75881, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/10\n",
      "1576/1576 [==============================] - 288s 183ms/step - loss: 6.6549 - accuracy: 0.0507\n",
      "\n",
      "Epoch 00004: loss improved from 6.75881 to 6.65263, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/10\n",
      "1576/1576 [==============================] - 284s 180ms/step - loss: 6.5593 - accuracy: 0.0532\n",
      "\n",
      "Epoch 00005: loss improved from 6.65263 to 6.55623, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/10\n",
      "1576/1576 [==============================] - 299s 190ms/step - loss: 6.4520 - accuracy: 0.0557\n",
      "\n",
      "Epoch 00006: loss improved from 6.55623 to 6.46000, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/10\n",
      "1576/1576 [==============================] - 288s 183ms/step - loss: 6.3484 - accuracy: 0.0569\n",
      "\n",
      "Epoch 00007: loss improved from 6.46000 to 6.35955, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/10\n",
      "1576/1576 [==============================] - 283s 179ms/step - loss: 6.2550 - accuracy: 0.0596\n",
      "\n",
      "Epoch 00008: loss improved from 6.35955 to 6.26244, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/10\n",
      "1576/1576 [==============================] - 286s 181ms/step - loss: 6.1517 - accuracy: 0.0611\n",
      "\n",
      "Epoch 00009: loss improved from 6.26244 to 6.16538, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/10\n",
      "1576/1576 [==============================] - 285s 181ms/step - loss: 6.0539 - accuracy: 0.0623\n",
      "\n",
      "Epoch 00010: loss improved from 6.16538 to 6.06628, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9016da7350>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=10, batch_size=100, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4304, 36, 706, 380]\n",
      "(1, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "pattern = [6,\n",
    "  4304,\n",
    "  36,\n",
    "  706,\n",
    "  380,]\n",
    "print(pattern)\n",
    "#model.summary()\n",
    "x = np.reshape(pattern, (1,len(pattern), 1))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of', 'cdc', 'just', 'stated', 'anyone']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(word) for word in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n",
    "my_texts = sequence_to_text(pattern)\n",
    "my_texts\n",
    "#tokenizer.sequences_to_texts_generator([pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to']\n",
      "['the']\n",
      "['people']\n",
      "['news']\n",
      "['media']\n",
      "['the']\n",
      "['the']\n",
      "['united']\n",
      "['states']\n",
      "['and']\n",
      "['the']\n",
      "['american']\n",
      "['left']\n",
      "['hunt']\n",
      "['hunt']\n",
      "['the']\n",
      "['the']\n",
      "['democrats']\n",
      "['of']\n",
      "['the']\n",
      "['united']\n",
      "['states']\n",
      "['and']\n",
      "['the']\n",
      "['american']\n",
      "['left']\n",
      "['hunt']\n",
      "['hunt']\n",
      "['the']\n",
      "['the']\n",
      "['democrats']\n",
      "['of']\n",
      "['the']\n",
      "['united']\n",
      "['states']\n",
      "['and']\n",
      "['the']\n",
      "['american']\n",
      "['left']\n",
      "['hunt']\n",
      "['hunt']\n",
      "['the']\n",
      "['the']\n",
      "['democrats']\n",
      "['of']\n",
      "['the']\n",
      "['united']\n",
      "['states']\n",
      "['and']\n",
      "['the']\n",
      "[['of'], ['the'], ['cdc'], ['just'], ['stated'], ['to'], ['the'], ['people'], ['news'], ['media'], ['the'], ['the'], ['united'], ['states'], ['and'], ['the'], ['american'], ['left'], ['hunt'], ['hunt'], ['the'], ['the'], ['democrats'], ['of'], ['the'], ['united'], ['states'], ['and'], ['the'], ['american'], ['left'], ['hunt'], ['hunt'], ['the'], ['the'], ['democrats'], ['of'], ['the'], ['united'], ['states'], ['and'], ['the'], ['american'], ['left'], ['hunt'], ['hunt'], ['the'], ['the'], ['democrats'], ['of'], ['the'], ['united'], ['states'], ['and'], ['the']]\n"
     ]
    }
   ],
   "source": [
    "pattern = [6,1,4304,36,706,]\n",
    "#pattern = [random.randint(1,total_words) for x in range(5)]\n",
    "out = [sequence_to_text([value]) for value in pattern]\n",
    "for i in range(50):\n",
    "    x = np.reshape(pattern, (1,len(pattern), 1))\n",
    "    x = x/float(len(tokenizer.word_index.items())) #cast as float so it doesn't do int divid \n",
    "#    print(x.shape)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    #print(prediction[0][:100])\n",
    "    index = np.argmax(prediction[0]) + 1 #it goes zero to 10383 so add 1 for offset\n",
    "    #print(index)\n",
    "    print(sequence_to_text([index]))\n",
    "    \n",
    "  \n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    seq_in = [sequence_to_text([value]) for value in pattern]\n",
    "    #print(seq_in)\n",
    "    out.append(sequence_to_text([index]))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.5076477e-03 7.0406152e-03 4.3313252e-03 ... 3.4010055e-09\n",
      "  3.1294920e-09 3.4714198e-09]]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_text([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence_to_text(y_data)\n",
    "#y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
