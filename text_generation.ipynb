{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "generous-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lined-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "closing-benjamin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>insult</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10360</td>\n",
       "      <td>10358</td>\n",
       "      <td>10360</td>\n",
       "      <td>10360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1573</td>\n",
       "      <td>866</td>\n",
       "      <td>6729</td>\n",
       "      <td>5673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2020-10-12</td>\n",
       "      <td>the-media</td>\n",
       "      <td>Fake News</td>\n",
       "      <td>The Fake News Networks, those that knowingly h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>45</td>\n",
       "      <td>1287</td>\n",
       "      <td>431</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date     target     insult  \\\n",
       "count        10360      10358      10360   \n",
       "unique        1573        866       6729   \n",
       "top     2020-10-12  the-media  Fake News   \n",
       "freq            45       1287        431   \n",
       "\n",
       "                                                    tweet  \n",
       "count                                               10360  \n",
       "unique                                               5673  \n",
       "top     The Fake News Networks, those that knowingly h...  \n",
       "freq                                                   16  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('trump_insult_tweets_2014_to_2021.csv',index_col='Unnamed: 0')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adapted-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweets'] = df['tweet'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "graphic-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = np.array(df['clean_tweets'][df['clean_tweets'] != ''].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-transsexual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "further-penalty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5254"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([len(tweet) for tweet in clean_tweets]).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-stamp",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "irish-belgium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   8,  10, ..., 281, 283, 284])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.array([len(tweet) for tweet in df['clean_tweets'].unique()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "operational-combining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Can you believe this fool, Dr. Thomas Frieden of CDC, just stated, \"anyone with fever should be asked if they have been in West Africa\" DOPE, Big time in U.S. today - MAKE AMERICA GREAT AGAIN! Politicians are all talk and no action - they can never bring us back., Politician @SenatorCardin didn't like that I said Baltimore needs jobs & spirit. It's politicians like Cardin that have destroyed Baltimore., For the nonbeliever, here is a photo of @Neilyoung in my office and his $$ request—total hypocrite. , .@Neilyoung’s song, “Rockin’ In The Free World” was just one of 10 songs used as background music. Didn’t love it anyway., Uncomfortable looking NBC reporter Willie Geist calls me to ask for favors and then mockingly smiles when he is told of my high poll numbers, Just out, the new nationwide @FoxNews poll has me alone in 2nd place, closely behind Jeb Bush-but Bush will NEVER Make America Great Again!, The ratings for The View are really low. Nicole Wallace and Molly Sims are a disaster. Get new cast or just put it to sleep. Dead T.V., .@WhoopiGoldberg had better surround herself with better hosts than Nicole Wallace, who doesn't have a clue. The show is close to death!, I hear that dopey political pundit, Lawrence O'Donnell, one of the dumber people on television, is about to lose his show-no ratings?Too bad]\n"
     ]
    }
   ],
   "source": [
    "docs = [nlp(x) for x in clean_tweets]\n",
    "print(docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "african-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[token for token in docs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "explicit-oxygen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "occasional-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "round-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-handy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "active-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    for j in range(len(doc) - seq_len):\n",
    "        \n",
    "        in_seq = doc[j : j + seq_len]\n",
    "        out_seq = doc[j + seq_len]\n",
    "        X_data.append([doc.vocab.strings[token.text] for token in in_seq])        \n",
    "        y_data.append(doc.vocab.strings[out_seq.text])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "permanent-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2457247847353624091"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vocab.strings[str(y_data[100])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "anticipated-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patterns = len(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "limited-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X_data, (n_patterns, seq_len, 1))\n",
    "y = np.asarray(pd.get_dummies(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fabulous-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "colonial-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "offensive-poverty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "811/811 [==============================] - 112s 135ms/step - loss: 7.1253\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.87142, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/30\n",
      "811/811 [==============================] - 109s 135ms/step - loss: 6.7249\n",
      "\n",
      "Epoch 00002: loss improved from 6.87142 to 6.73362, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/30\n",
      "811/811 [==============================] - 105s 129ms/step - loss: 6.7144\n",
      "\n",
      "Epoch 00003: loss improved from 6.73362 to 6.72474, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/30\n",
      "811/811 [==============================] - 87s 107ms/step - loss: 6.7192\n",
      "\n",
      "Epoch 00004: loss improved from 6.72474 to 6.72189, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.7070\n",
      "\n",
      "Epoch 00005: loss improved from 6.72189 to 6.72009, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.7051\n",
      "\n",
      "Epoch 00006: loss improved from 6.72009 to 6.71960, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.7200\n",
      "\n",
      "Epoch 00007: loss improved from 6.71960 to 6.71717, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.7012\n",
      "\n",
      "Epoch 00008: loss improved from 6.71717 to 6.71595, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/30\n",
      "811/811 [==============================] - 87s 107ms/step - loss: 6.6950\n",
      "\n",
      "Epoch 00009: loss improved from 6.71595 to 6.71493, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/30\n",
      "811/811 [==============================] - 87s 107ms/step - loss: 6.7034\n",
      "\n",
      "Epoch 00010: loss improved from 6.71493 to 6.71321, saving model to model_weights_saved.hdf5\n",
      "Epoch 11/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.6945\n",
      "\n",
      "Epoch 00011: loss improved from 6.71321 to 6.71260, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.7025\n",
      "\n",
      "Epoch 00012: loss improved from 6.71260 to 6.71163, saving model to model_weights_saved.hdf5\n",
      "Epoch 13/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.7097\n",
      "\n",
      "Epoch 00013: loss improved from 6.71163 to 6.71027, saving model to model_weights_saved.hdf5\n",
      "Epoch 14/30\n",
      "811/811 [==============================] - 87s 107ms/step - loss: 6.6991\n",
      "\n",
      "Epoch 00014: loss improved from 6.71027 to 6.70964, saving model to model_weights_saved.hdf5\n",
      "Epoch 15/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.6979\n",
      "\n",
      "Epoch 00015: loss improved from 6.70964 to 6.70864, saving model to model_weights_saved.hdf5\n",
      "Epoch 16/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.6927\n",
      "\n",
      "Epoch 00016: loss improved from 6.70864 to 6.70856, saving model to model_weights_saved.hdf5\n",
      "Epoch 17/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.6973\n",
      "\n",
      "Epoch 00017: loss improved from 6.70856 to 6.70759, saving model to model_weights_saved.hdf5\n",
      "Epoch 18/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.7001\n",
      "\n",
      "Epoch 00018: loss improved from 6.70759 to 6.70685, saving model to model_weights_saved.hdf5\n",
      "Epoch 19/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.7045\n",
      "\n",
      "Epoch 00019: loss improved from 6.70685 to 6.70619, saving model to model_weights_saved.hdf5\n",
      "Epoch 20/30\n",
      "811/811 [==============================] - 86s 106ms/step - loss: 6.7157\n",
      "\n",
      "Epoch 00020: loss improved from 6.70619 to 6.70541, saving model to model_weights_saved.hdf5\n",
      "Epoch 21/30\n",
      "811/811 [==============================] - 86s 106ms/step - loss: 6.6889\n",
      "\n",
      "Epoch 00021: loss improved from 6.70541 to 6.70510, saving model to model_weights_saved.hdf5\n",
      "Epoch 22/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.7091\n",
      "\n",
      "Epoch 00022: loss improved from 6.70510 to 6.70437, saving model to model_weights_saved.hdf5\n",
      "Epoch 23/30\n",
      "811/811 [==============================] - 86s 106ms/step - loss: 6.6967\n",
      "\n",
      "Epoch 00023: loss improved from 6.70437 to 6.70409, saving model to model_weights_saved.hdf5\n",
      "Epoch 24/30\n",
      "811/811 [==============================] - 86s 106ms/step - loss: 6.6951\n",
      "\n",
      "Epoch 00024: loss improved from 6.70409 to 6.70357, saving model to model_weights_saved.hdf5\n",
      "Epoch 25/30\n",
      "811/811 [==============================] - 86s 106ms/step - loss: 6.6994\n",
      "\n",
      "Epoch 00025: loss improved from 6.70357 to 6.70303, saving model to model_weights_saved.hdf5\n",
      "Epoch 26/30\n",
      "811/811 [==============================] - 87s 107ms/step - loss: 6.6987\n",
      "\n",
      "Epoch 00026: loss improved from 6.70303 to 6.70262, saving model to model_weights_saved.hdf5\n",
      "Epoch 27/30\n",
      "811/811 [==============================] - 87s 107ms/step - loss: 6.6993\n",
      "\n",
      "Epoch 00027: loss did not improve from 6.70262\n",
      "Epoch 28/30\n",
      "811/811 [==============================] - 86s 106ms/step - loss: 6.6851\n",
      "\n",
      "Epoch 00028: loss improved from 6.70262 to 6.70207, saving model to model_weights_saved.hdf5\n",
      "Epoch 29/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.6960\n",
      "\n",
      "Epoch 00029: loss improved from 6.70207 to 6.70178, saving model to model_weights_saved.hdf5\n",
      "Epoch 30/30\n",
      "811/811 [==============================] - 86s 107ms/step - loss: 6.7012\n",
      "\n",
      "Epoch 00030: loss improved from 6.70178 to 6.70131, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe284066e50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=30, batch_size=256, callbacks=desired_callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "second-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [doc.vocab.strings['I'], doc.vocab.strings['want']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "emotional-respect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1757\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '1757'. This usually refers to an issue with the `Vocab` or `StringStore`.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b97310829836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mseq_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# CONVERT INDEX TO COL NAME TO RESTORE SPACY HASH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/strings.pyx\u001b[0m in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '1757'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "#     x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    print(index)\n",
    "    result = doc.vocab.strings[index]\n",
    "    seq_in = [doc.vocab.strings[value] for value in pattern]\n",
    "    # CONVERT INDEX TO COL NAME TO RESTORE SPACY HASH \n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-japanese",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
