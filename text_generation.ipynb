{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "impressive-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "artificial-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "controlled-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continent-logging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "      <th>insult</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10360</td>\n",
       "      <td>10358</td>\n",
       "      <td>10360</td>\n",
       "      <td>10360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1573</td>\n",
       "      <td>866</td>\n",
       "      <td>6729</td>\n",
       "      <td>5673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2020-10-12</td>\n",
       "      <td>the-media</td>\n",
       "      <td>Fake News</td>\n",
       "      <td>The Fake News Networks, those that knowingly h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>45</td>\n",
       "      <td>1287</td>\n",
       "      <td>431</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date     target     insult  \\\n",
       "count        10360      10358      10360   \n",
       "unique        1573        866       6729   \n",
       "top     2020-10-12  the-media  Fake News   \n",
       "freq            45       1287        431   \n",
       "\n",
       "                                                    tweet  \n",
       "count                                               10360  \n",
       "unique                                               5673  \n",
       "top     The Fake News Networks, those that knowingly h...  \n",
       "freq                                                   16  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('trump_insult_tweets_2014_to_2021.csv',index_col='Unnamed: 0')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "understanding-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweets'] = df['tweet'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "designing-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = np.array(df['clean_tweets'][df['clean_tweets'] != ''].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-retailer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "applicable-industry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5254"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([len(tweet) for tweet in clean_tweets]).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-spray",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brutal-specialist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   8,  10, ..., 281, 283, 284])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.array([len(tweet) for tweet in df['clean_tweets'].unique()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exterior-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Can you believe this fool, Dr. Thomas Frieden of CDC, just stated, \"anyone with fever should be asked if they have been in West Africa\" DOPE, Big time in U.S. today - MAKE AMERICA GREAT AGAIN! Politicians are all talk and no action - they can never bring us back., Politician @SenatorCardin didn't like that I said Baltimore needs jobs & spirit. It's politicians like Cardin that have destroyed Baltimore., For the nonbeliever, here is a photo of @Neilyoung in my office and his $$ request‚Äîtotal hypocrite. , .@Neilyoung‚Äôs song, ‚ÄúRockin‚Äô In The Free World‚Äù was just one of 10 songs used as background music. Didn‚Äôt love it anyway., Uncomfortable looking NBC reporter Willie Geist calls me to ask for favors and then mockingly smiles when he is told of my high poll numbers, Just out, the new nationwide @FoxNews poll has me alone in 2nd place, closely behind Jeb Bush-but Bush will NEVER Make America Great Again!, The ratings for The View are really low. Nicole Wallace and Molly Sims are a disaster. Get new cast or just put it to sleep. Dead T.V., .@WhoopiGoldberg had better surround herself with better hosts than Nicole Wallace, who doesn't have a clue. The show is close to death!, I hear that dopey political pundit, Lawrence O'Donnell, one of the dumber people on television, is about to lose his show-no ratings?Too bad]\n"
     ]
    }
   ],
   "source": [
    "docs = [nlp(x) for x in clean_tweets]\n",
    "print(docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "operating-proposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, '&': 7, \"'\": 8, '(': 9, ')': 10, '+': 11, ',': 12, '-': 13, '.': 14, '/': 15, '0': 16, '1': 17, '2': 18, '3': 19, '4': 20, '5': 21, '6': 22, '7': 23, '8': 24, '9': 25, ':': 26, '=': 27, '?': 28, '@': 29, 'A': 30, 'B': 31, 'C': 32, 'D': 33, 'E': 34, 'F': 35, 'G': 36, 'H': 37, 'I': 38, 'J': 39, 'K': 40, 'L': 41, 'M': 42, 'N': 43, 'O': 44, 'P': 45, 'Q': 46, 'R': 47, 'S': 48, 'T': 49, 'U': 50, 'V': 51, 'W': 52, 'X': 53, 'Y': 54, 'Z': 55, '_': 56, 'a': 57, 'b': 58, 'c': 59, 'd': 60, 'e': 61, 'f': 62, 'g': 63, 'h': 64, 'i': 65, 'j': 66, 'k': 67, 'l': 68, 'm': 69, 'n': 70, 'o': 71, 'p': 72, 'q': 73, 'r': 74, 's': 75, 't': 76, 'u': 77, 'v': 78, 'w': 79, 'x': 80, 'y': 81, 'z': 82, '{': 83, '}': 84, '\\x8f': 85, '\\x9d': 86, '\\xa0': 87, '¬°': 88, '¬¶': 89, '¬µ': 90, '¬∑': 91, '¬∏': 92, '¬∫': 93, '¬Ω': 94, '¬ø': 95, '√Ñ': 96, '√¢': 97, '√©': 98, '√Ø': 99, '√∞': 100, '√¥': 101, '√∂': 102, '√π': 103, '√∫': 104, '≈í': 105, '≈∏': 106, '≈æ': 107, 'Àú': 108, '‚Äì': 109, '‚Äî': 110, '‚Äò': 111, '‚Äô': 112, '‚Äö': 113, '‚Äú': 114, '‚Äù': 115, '‚Ä°': 116, '‚Ä¶': 117, '‚Ç¨': 118, 'üòâ': 119, 'üòß': 120}\n"
     ]
    }
   ],
   "source": [
    "tot_chars = [list(set(str(doc))) for doc in docs]\n",
    "tot_chars = list(itertools.chain.from_iterable(tot_chars))\n",
    "chars = sorted(list(set(tot_chars)))\n",
    "\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))\n",
    "print(char_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "affiliated-testing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193290 121\n"
     ]
    }
   ],
   "source": [
    "input_len = len(tot_chars)\n",
    "vocab_len = len(chars)\n",
    "print(input_len, vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ranging-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "comprehensive-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "great-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, input_len - seq_len, 1):\n",
    "    in_seq = tot_chars[i:i + seq_len]\n",
    "\n",
    "    # Out sequence is the initial character plus total sequence length\n",
    "    out_seq = tot_chars[i + seq_len]\n",
    "\n",
    "    # We now convert list of characters to integers based on\n",
    "    # previously and add the values to our lists\n",
    "    X_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "qualified-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in docs:\n",
    "#     for j in range(len(doc) - seq_len):\n",
    "        \n",
    "#         in_seq = doc[j : j + seq_len]\n",
    "#         out_seq = doc[j + seq_len]\n",
    "        \n",
    "#         X_data = np.append(X_data, [twts_to_num[char] for char in in_seq])\n",
    "#         y_data.append(doc.vocab.strings[out_seq.text])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "designed-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc.vocab.strings[str(y_data[100])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "automated-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patterns = len(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "charitable-financing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193288"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "considered-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X_data, (n_patterns, seq_len, 1))\n",
    "X = X/float(vocab_len)\n",
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "found-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.reshape(X_data, (n_patterns, seq_len, 1))\n",
    "# X = X / float(len(doc.vocab))\n",
    "# y_dum = pd.get_dummies(y_data)\n",
    "# cols = y_dum.columns\n",
    "# y = np.asarray(y_dum)\n",
    "# print(cols, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "mineral-calculation",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ee698dcfa7ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cols' is not defined"
     ]
    }
   ],
   "source": [
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sapphire-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256, return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "binding-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"model_weights_saved.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faced-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "filepath = \"model_weights_saved4.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "wooden-large",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 2, 256)            264192    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 121)               15609     \n",
      "=================================================================\n",
      "Total params: 476,921\n",
      "Trainable params: 476,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "attended-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1511/1511 [==============================] - 32s 20ms/step - loss: 3.6977\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.14287, saving model to model_weights_saved4.hdf5\n",
      "Epoch 2/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 2.1320\n",
      "\n",
      "Epoch 00002: loss improved from 3.14287 to 2.00787, saving model to model_weights_saved4.hdf5\n",
      "Epoch 3/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.7617\n",
      "\n",
      "Epoch 00003: loss improved from 2.00787 to 1.71559, saving model to model_weights_saved4.hdf5\n",
      "Epoch 4/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.6086\n",
      "\n",
      "Epoch 00004: loss improved from 1.71559 to 1.57954, saving model to model_weights_saved4.hdf5\n",
      "Epoch 5/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.5189\n",
      "\n",
      "Epoch 00005: loss improved from 1.57954 to 1.50282, saving model to model_weights_saved4.hdf5\n",
      "Epoch 6/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.4590\n",
      "\n",
      "Epoch 00006: loss improved from 1.50282 to 1.44909, saving model to model_weights_saved4.hdf5\n",
      "Epoch 7/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.4343\n",
      "\n",
      "Epoch 00007: loss improved from 1.44909 to 1.41471, saving model to model_weights_saved4.hdf5\n",
      "Epoch 8/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.3988\n",
      "\n",
      "Epoch 00008: loss improved from 1.41471 to 1.38936, saving model to model_weights_saved4.hdf5\n",
      "Epoch 9/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.3731\n",
      "\n",
      "Epoch 00009: loss improved from 1.38936 to 1.36803, saving model to model_weights_saved4.hdf5\n",
      "Epoch 10/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.3516\n",
      "\n",
      "Epoch 00010: loss improved from 1.36803 to 1.35059, saving model to model_weights_saved4.hdf5\n",
      "Epoch 11/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.3374\n",
      "\n",
      "Epoch 00011: loss improved from 1.35059 to 1.33485, saving model to model_weights_saved4.hdf5\n",
      "Epoch 12/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.3227\n",
      "\n",
      "Epoch 00012: loss improved from 1.33485 to 1.32250, saving model to model_weights_saved4.hdf5\n",
      "Epoch 13/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.3063\n",
      "\n",
      "Epoch 00013: loss improved from 1.32250 to 1.30921, saving model to model_weights_saved4.hdf5\n",
      "Epoch 14/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.3020\n",
      "\n",
      "Epoch 00014: loss improved from 1.30921 to 1.30384, saving model to model_weights_saved4.hdf5\n",
      "Epoch 15/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2908\n",
      "\n",
      "Epoch 00015: loss improved from 1.30384 to 1.29334, saving model to model_weights_saved4.hdf5\n",
      "Epoch 16/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2806\n",
      "\n",
      "Epoch 00016: loss improved from 1.29334 to 1.28085, saving model to model_weights_saved4.hdf5\n",
      "Epoch 17/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2780\n",
      "\n",
      "Epoch 00017: loss improved from 1.28085 to 1.27444, saving model to model_weights_saved4.hdf5\n",
      "Epoch 18/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2788\n",
      "\n",
      "Epoch 00018: loss improved from 1.27444 to 1.26979, saving model to model_weights_saved4.hdf5\n",
      "Epoch 19/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2622\n",
      "\n",
      "Epoch 00019: loss improved from 1.26979 to 1.26374, saving model to model_weights_saved4.hdf5\n",
      "Epoch 20/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2620\n",
      "\n",
      "Epoch 00020: loss improved from 1.26374 to 1.25579, saving model to model_weights_saved4.hdf5\n",
      "Epoch 21/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2465\n",
      "\n",
      "Epoch 00021: loss improved from 1.25579 to 1.25014, saving model to model_weights_saved4.hdf5\n",
      "Epoch 22/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2476\n",
      "\n",
      "Epoch 00022: loss improved from 1.25014 to 1.24694, saving model to model_weights_saved4.hdf5\n",
      "Epoch 23/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2416\n",
      "\n",
      "Epoch 00023: loss improved from 1.24694 to 1.24231, saving model to model_weights_saved4.hdf5\n",
      "Epoch 24/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2395\n",
      "\n",
      "Epoch 00024: loss improved from 1.24231 to 1.23730, saving model to model_weights_saved4.hdf5\n",
      "Epoch 25/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2416\n",
      "\n",
      "Epoch 00025: loss improved from 1.23730 to 1.23578, saving model to model_weights_saved4.hdf5\n",
      "Epoch 26/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2327\n",
      "\n",
      "Epoch 00026: loss improved from 1.23578 to 1.23246, saving model to model_weights_saved4.hdf5\n",
      "Epoch 27/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2249\n",
      "\n",
      "Epoch 00027: loss improved from 1.23246 to 1.22477, saving model to model_weights_saved4.hdf5\n",
      "Epoch 28/50\n",
      "1511/1511 [==============================] - 29s 20ms/step - loss: 1.2159\n",
      "\n",
      "Epoch 00028: loss improved from 1.22477 to 1.22381, saving model to model_weights_saved4.hdf5\n",
      "Epoch 29/50\n",
      "1511/1511 [==============================] - 29s 20ms/step - loss: 1.2211\n",
      "\n",
      "Epoch 00029: loss improved from 1.22381 to 1.22263, saving model to model_weights_saved4.hdf5\n",
      "Epoch 30/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2131\n",
      "\n",
      "Epoch 00030: loss improved from 1.22263 to 1.21587, saving model to model_weights_saved4.hdf5\n",
      "Epoch 31/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2128\n",
      "\n",
      "Epoch 00031: loss improved from 1.21587 to 1.21217, saving model to model_weights_saved4.hdf5\n",
      "Epoch 32/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2157\n",
      "\n",
      "Epoch 00032: loss improved from 1.21217 to 1.20950, saving model to model_weights_saved4.hdf5\n",
      "Epoch 33/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2737\n",
      "\n",
      "Epoch 00033: loss did not improve from 1.20950\n",
      "Epoch 34/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1982\n",
      "\n",
      "Epoch 00034: loss improved from 1.20950 to 1.20171, saving model to model_weights_saved4.hdf5\n",
      "Epoch 35/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2181\n",
      "\n",
      "Epoch 00035: loss did not improve from 1.20171\n",
      "Epoch 36/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1948\n",
      "\n",
      "Epoch 00036: loss improved from 1.20171 to 1.19580, saving model to model_weights_saved4.hdf5\n",
      "Epoch 37/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2021\n",
      "\n",
      "Epoch 00037: loss did not improve from 1.19580\n",
      "Epoch 38/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1958\n",
      "\n",
      "Epoch 00038: loss improved from 1.19580 to 1.19248, saving model to model_weights_saved4.hdf5\n",
      "Epoch 39/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1996\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.19248\n",
      "Epoch 40/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1895\n",
      "\n",
      "Epoch 00040: loss improved from 1.19248 to 1.18984, saving model to model_weights_saved4.hdf5\n",
      "Epoch 41/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.2035\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.18984\n",
      "Epoch 42/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1856\n",
      "\n",
      "Epoch 00042: loss improved from 1.18984 to 1.18189, saving model to model_weights_saved4.hdf5\n",
      "Epoch 43/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1783\n",
      "\n",
      "Epoch 00043: loss did not improve from 1.18189\n",
      "Epoch 44/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1800\n",
      "\n",
      "Epoch 00044: loss improved from 1.18189 to 1.17971, saving model to model_weights_saved4.hdf5\n",
      "Epoch 45/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1806\n",
      "\n",
      "Epoch 00045: loss did not improve from 1.17971\n",
      "Epoch 46/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1767\n",
      "\n",
      "Epoch 00046: loss improved from 1.17971 to 1.17429, saving model to model_weights_saved4.hdf5\n",
      "Epoch 47/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1799\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.17429\n",
      "Epoch 48/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1675\n",
      "\n",
      "Epoch 00048: loss improved from 1.17429 to 1.17147, saving model to model_weights_saved4.hdf5\n",
      "Epoch 49/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1659\n",
      "\n",
      "Epoch 00049: loss improved from 1.17147 to 1.16924, saving model to model_weights_saved4.hdf5\n",
      "Epoch 50/50\n",
      "1511/1511 [==============================] - 30s 20ms/step - loss: 1.1656\n",
      "\n",
      "Epoch 00050: loss improved from 1.16924 to 1.16884, saving model to model_weights_saved4.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc0847330d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=128, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "above-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "intense-degree",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" H? \"\n"
     ]
    }
   ],
   "source": [
    "start = np.random.randint(0, len(X_data) - 1)\n",
    "pattern = X_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "moved-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = [doc.vocab.strings['We'], doc.vocab.strings['love']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "supposed-remove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 28]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "respected-royal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr uvonwTgf.yaDtdkshASpliebCUIc!,Fmr uvonwTgf.yaDtdkshASpliebCUIc!,Fmr uvonwTgf.yaDtdkshASpliebCUIc!\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "for i in range(100):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "    res.append(result)\n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print(\"\".join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "selected-doctor",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-34bc52fc2b3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(doc.vocab))\n",
    "    \n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    print(prediction)\n",
    "    index = np.argmax(prediction[0])\n",
    "    print(index)\n",
    "    result = doc.vocab.strings[cols[index]]\n",
    "    seq_in = [doc.vocab.strings[value] for value in pattern]\n",
    "    # CONVERT INDEX TO COL NAME TO RESTORE SPACY HASH\n",
    "    print(seq_in, \"----\", result)\n",
    "\n",
    "    pattern.append(cols[index])\n",
    "    pattern = pattern[1:len(pattern)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-guitar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
